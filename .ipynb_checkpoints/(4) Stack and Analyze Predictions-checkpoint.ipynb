{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a6e692",
   "metadata": {},
   "source": [
    "The purpose of this code is to gather the predicted images, load them into their original volume and calculate the dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6064f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from keras_unet.metrics import dice_coef\n",
    "from keras_unet.models import custom_unet\n",
    "from keras_unet.losses import jaccard_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import fnmatch\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802c37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "smooth = 1\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100): \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(K.abs(y_true_f * y_pred_f)) \n",
    "    sum_ = K.sum(K.abs(y_true_f) + K.abs(y_pred_f)) \n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth) \n",
    "    return (1 - jac) * smooth \n",
    "\n",
    "def mean_length_error(y_true, y_pred):\n",
    "    y_true_f = K.sum(K.round(K.flatten(y_true)))\n",
    "    y_pred_f = K.sum(K.round(K.flatten(y_pred)))\n",
    "    delta = (y_pred_f - y_true_f)\n",
    "    return K.mean(K.tanh(delta))\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def np_dice_coef(y_true, y_pred):\n",
    "    tr = y_true.flatten()\n",
    "    pr = y_pred.flatten()\n",
    "    return (2. * np.sum(tr * pr) + smooth) / (np.sum(tr) + np.sum(pr) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb67528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_set(data_path, phrase):\n",
    "    set_of = []\n",
    "    path = data_path + '\\\\'\n",
    "    for f in os.listdir(data_path):\n",
    "      if phrase in f:\n",
    "        set_of.append(f)\n",
    "      else:\n",
    "        continue\n",
    "    set_of = np.array(set_of)\n",
    "\n",
    "    indices = np.array(range(len(set_of))) # we will use this in the next step.\n",
    "\n",
    "    return set_of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0158d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_predictions = r\"E:\\Kidney Unet\\Mayo Cross 4 predictions\"\n",
    "filepath_tensors = r\"E:\\Kidney Unet\\tensors\"\n",
    "filepath_data = r\"E:\\Kidney Unet\\Mayo Small Batch data\"\n",
    "images = gather_set(filepath_predictions, 'P')\n",
    "model_name = 'UNET_MA_KU_UB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cb1b4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []   \n",
    "for i in range(len(images)):\n",
    "    image_name = images[i]\n",
    "    unique_id =  image_name[0:17]\n",
    "    id_list.append(unique_id)\n",
    "unique_ids = list(set(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "597e91c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MA_300641_1_108_L', 'MA_300641_2_99_R_', 'MA_300641_1_108_R', 'MA_300641_2_99_L_', 'MA_300641_0_105_R', 'MA_300641_0_105_L', 'MA_300641_3_120_L', 'MA_300641_3_120_R']\n"
     ]
    }
   ],
   "source": [
    "print(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f55bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unique_ids)):\n",
    "    name = unique_ids[i]\n",
    "    if not name.endswith('_'):\n",
    "        name = name+'_'\n",
    "        unique_ids[i]=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b25c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MA_300641_1_108_L_', 'MA_300641_2_99_R_', 'MA_300641_1_108_R_', 'MA_300641_2_99_L_', 'MA_300641_0_105_R_', 'MA_300641_0_105_L_', 'MA_300641_3_120_L_', 'MA_300641_3_120_R_']\n"
     ]
    }
   ],
   "source": [
    "print(unique_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055505a",
   "metadata": {},
   "source": [
    "Stack original and predicted images into a tensor for metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f538ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unique_ids)):\n",
    "    pt_info = unique_ids[i]\n",
    "    pt_num, yr_num, num_slices = re.findall(r'\\d+', pt_info)\n",
    "    tensor = np.zeros((512,512,int(num_slices)))\n",
    "    for x in range(int(num_slices)):\n",
    "        img_name = unique_ids[i]+str(x)+'_K.npy'\n",
    "        image = np.load(filepath_data + '\\\\' + img_name)\n",
    "        img_slice = image\n",
    "        tensor[:,:,x] = img_slice\n",
    "        x = x+1\n",
    "    new_fname = unique_ids[i]+'K.npy'\n",
    "    np.save(os.path.join(filepath_tensors, new_fname), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b5628fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unique_ids)):\n",
    "    pt_info = unique_ids[i]\n",
    "    pt_num, yr_num, num_slices = re.findall(r'\\d+', pt_info)\n",
    "    tensor = np.zeros((512,512,int(num_slices)))\n",
    "    for x in range(int(num_slices)):\n",
    "        img_name = unique_ids[i]+str(x)+ '_' + model_name +'_P.npy'\n",
    "        image = np.load(filepath_predictions + '\\\\' + img_name)\n",
    "        img_slice = image[:,:,1]\n",
    "        tensor[:,:,x] = img_slice\n",
    "        x = x+1\n",
    "    new_fname = unique_ids[i]+ model_name +'_Prediction.npy'\n",
    "    np.save(os.path.join(filepath_tensors, new_fname), tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19659c4",
   "metadata": {},
   "source": [
    "Gather prediction tensors and calculate stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4bca7ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KU_101934_3_96_R_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_L_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_1_96_L_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_L_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_L_UNET_MA_KU_EM_Prediction.npy'\n",
      " 'KU_101934_1_96_L_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_R_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_1_96_R_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_R_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_1_96_R_UNET_MA_KU_EM_Prediction.npy'\n",
      " 'KU_101934_1_96_R_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_L_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_2_96_L_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_L_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_L_UNET_MA_KU_EM_Prediction.npy'\n",
      " 'KU_101934_2_96_L_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_R_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_2_96_R_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_R_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_2_96_R_UNET_MA_KU_EM_Prediction.npy'\n",
      " 'KU_101934_2_96_R_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_L_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_3_96_L_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_L_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_L_UNET_MA_KU_EM_Prediction.npy'\n",
      " 'KU_101934_3_96_L_UNET_MA_KU_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_R_ALL_INSTITUTION_80-20_Prediction.npy'\n",
      " 'KU_101934_3_96_R_UNET_KU_EM_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_R_UNET_MA_EM_UB_Prediction.npy'\n",
      " 'KU_101934_3_96_R_UNET_MA_KU_EM_Prediction.npy']\n",
      "['KU_101934_2_96_L_K.npy' 'KU_101934_2_96_R_K.npy'\n",
      " 'KU_101934_3_96_L_K.npy' 'KU_101934_3_96_R_K.npy'\n",
      " 'KU_101934_1_96_L_K.npy' 'KU_101934_1_96_R_K.npy']\n"
     ]
    }
   ],
   "source": [
    "filepath_tensors = r'E:\\Kidney Unet\\tensors\\KU'\n",
    "pred_list = gather_set(filepath_tensors, '_Prediction')\n",
    "true_list = gather_set(filepath_tensors, '_K.')\n",
    "print(pred_list)\n",
    "print(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "39843b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET_KU_EM_UB_\n",
      "MA_300641_3_120_L_K.npy\n"
     ]
    }
   ],
   "source": [
    "name =pred_list[10]\n",
    "print(name[18:32])\n",
    "test = pred_list[12][:17]+'_K.npy'\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fce80b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Kidney Unet\\\\tensors\\\\Mayo\\\\MA_300641_2_99_R__K.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-60478cf3baf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_tensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_tensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_K.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdice_calc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdice_coef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF23\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\Kidney Unet\\\\tensors\\\\Mayo\\\\MA_300641_2_99_R__K.npy'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(pred_list)):\n",
    "    prediction = np.load(filepath_tensors + '\\\\'+ pred_list[i])\n",
    "    true = np.load(filepath_tensors + '\\\\'+pred_list[i][:17]+'_K.npy')\n",
    "    dice_calc = dice_coef(true,prediction)\n",
    "    model = pred_list[i][18:32]\n",
    "    patient = pred_list[i][:17]\n",
    "    new_calc = [patient, model, dice_calc.numpy()]\n",
    "    results.append(new_calc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ba0e9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MA_300641_0_105_R', 'UNET_KU_EM_UB_', 0.9404108879220466], ['MA_300641_0_105_L', 'UNET_KU_EM_UB_', 0.9153872481642252], ['MA_300641_0_105_R', 'UNET_MA_KU_UB_', 0.9559004415679583], ['MA_300641_0_105_R', 'UNET_MA_KU_EM_', 0.9672008989305677], ['MA_300641_0_105_R', 'UNET_MA_EM_UB_', 0.9589584789588653], ['MA_300641_0_105_L', 'UNET_MA_KU_UB_', 0.9633882110852762], ['MA_300641_0_105_L', 'UNET_MA_KU_EM_', 0.9670094426627079], ['MA_300641_0_105_L', 'UNET_MA_EM_UB_', 0.9563058052895345]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e273351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns =['Patient Number', 'Model', 'Dice Score']\n",
    "filepath = r\"E:\\Kidney Unet\\tensors\\Mayo\\Mayo-results-Updated.xlsx\"\n",
    "df.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f29ad",
   "metadata": {},
   "source": [
    "Compute mean square distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff7d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import morphology\n",
    "#https://mlnotebook.github.io/post/surface-distance-function/\n",
    "def surfd(input1, input2, sampling=1, connectivity=1):\n",
    "    \n",
    "    input_1 = np.atleast_1d(input1.astype(np.bool))\n",
    "    input_2 = np.atleast_1d(input2.astype(np.bool))\n",
    "    \n",
    "\n",
    "    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)\n",
    "\n",
    "    S = input_1 - morphology.binary_erosion(input_1, conn)\n",
    "    Sprime = input_2 - morphology.binary_erosion(input_2, conn)\n",
    "\n",
    "    \n",
    "    dta = morphology.distance_transform_edt(~S,sampling)\n",
    "    dtb = morphology.distance_transform_edt(~Sprime,sampling)\n",
    "    \n",
    "    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])\n",
    "       \n",
    "    \n",
    "    return sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08fae247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-a5e6382c6382>:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_1 = np.atleast_1d(input1.astype(np.bool))\n",
      "<ipython-input-14-a5e6382c6382>:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_2 = np.atleast_1d(input2.astype(np.bool))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-55684d780e48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_srf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurfd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdice_test_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdice_test_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-a5e6382c6382>\u001b[0m in \u001b[0;36msurfd\u001b[1;34m(input1, input2, sampling, connectivity)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_binary_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_erosion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mSprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_erosion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "results_srf = surfd(dice_test_true,dice_test_pred,sampling=1, connectivity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0348bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather images that have _K and _P \n",
    "\n",
    "\n",
    "#gather groups that have the first xx number of characters the same\n",
    "\n",
    "#loop through to put all into a 3D tensor\n",
    "#analyze with dice_coef \n",
    "#analyze with surfd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF23",
   "language": "python",
   "name": "tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
